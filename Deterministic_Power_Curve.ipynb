{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vae9FMuIZObB"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import math\n","import numpy as np\n","import pandas as pd\n","import warnings\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import random\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.model_selection import KFold, cross_val_score, train_test_split\n","\n","import tensorflow_probability as tfp\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM\n","\n","from tensorflow.keras.layers import StringLookup\n","\n","#ignore warnings\n","warnings.filterwarnings(\"ignore\")\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eTlhuftDwMqT"},"outputs":[],"source":["# brewer2mpl.get_map args: set name set type number of colors\n","import matplotlib as mpl\n","\n","#mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=colors)\n","mpl.rcParams['font.family'] = 'Times New Roman'\n","\n","plt.rcParams['savefig.dpi'] = 300\n","plt.rcParams['figure.dpi'] = 300"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":30862,"status":"ok","timestamp":1650789954168,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"pFLQaynUZXVX","outputId":"153cb03a-de0e-4ee8-ac32-89bf17f0bfe9"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n#进行u-sd标准化\\ncol_name=speed_df.columns\\nfor item in col_name:\\n    tmp=np.array(speed_df[item])\\n    tmp_1=(tmp-np.mean(tmp))/np.std(tmp)\\n    speed_df[item]=tmp_1\\n\\npower=(power-np.min(power))/(np.max(power)-np.min(power))\\n'"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["data=pd.read_csv('data.csv')\n","item=3\n","\n","data['time']=pd.to_datetime(data['time'],format = '%Y-%m-%d %H:%M:%S')\n","\n","turbulence = np.zeros((len(data), 1))\n","\n","speed_for_turbulence = np.array(data['speed(m/s)_%s'%item])\n","\n","for i in range(5,len(data)-1):\n","  tmp = speed_for_turbulence[(i-5):(i+1)]\n","  turbulence[i][0] = np.std(tmp)/(np.mean(tmp)+0.001)\n","\n","turbulence[turbulence>1] = 1\n","data['turbulence_%s'%item]=turbulence\n","\n","angle=np.array(data['angle(°)_%s'%item])\n","cos_angle=[np.cos(np.pi*(i/180)) for i in angle]\n","data['angle(°)_%s'%item]=cos_angle\n","\n","norm_data_original=data[data['state_%s'%item]==10]\n","norm_data1=norm_data_original[norm_data_original['active_value_%s'%item]==100]\n","norm_data2=norm_data_original[norm_data_original['active_value_%s'%item]==0]\n","\n","norm_data=pd.concat([norm_data1,norm_data2])\n","\n","ab_data=data[data['state_%s'%item]!=10]\n","limit_data=pd.concat([norm_data_original,norm_data],axis =0).drop_duplicates(keep=False)\n","limit_data=pd.concat([limit_data,ab_data])\n","\n","speed_all=np.array(data['speed(m/s)_%s'%item])\n","power_all=np.array(data['power(kW)_%s'%item])\n","temperature_all=np.array(data['temperature(℃)_%s'%item])\n","angle_all=np.array(data['angle(°)_%s'%item])\n","turbulence_all=np.array(data['turbulence_%s'%item])\n","\n","\n","speed=np.array(norm_data['speed(m/s)_%s'%item])\n","power=np.array(norm_data['power(kW)_%s'%item])\n","temperature=np.array(norm_data['temperature(℃)_%s'%item])\n","angle=np.array(norm_data['angle(°)_%s'%item])\n","turbulence=np.array(norm_data['turbulence_%s'%item])\n","\n","speed_index=norm_data['speed(m/s)_%s'%item].index \n","\n","#generate dataset\n","speed_data=[[] for i in range(120)]\n","power_data=[[] for i in range(120)]\n","tem_data=[[] for i in range(120)]\n","angle_data=[[] for i in range(120)]\n","turbulence_data=[[] for i in range(120)]\n","\n","speed_index=norm_data['speed(m/s)_%s'%item].index\n","for i in speed_index:\n","    for j in range(120):\n","        speed_data[j].append(speed_all[i-j])\n","\n","power_index=norm_data['power(kW)_%s'%item].index\n","\n","for i in power_index:\n","    for j in range(120):\n","        power_data[j].append(power_all[i-j])\n","    \n","tem_index=norm_data['temperature(℃)_%s'%item].index\n","\n","for i in tem_index:\n","    for j in range(120):\n","        tem_data[j].append(temperature_all[i-j])\n","\n","angle_index=norm_data['angle(°)_%s'%item].index\n","\n","for i in angle_index:\n","    for j in range(120):\n","        angle_data[j].append(angle_all[i-j])\n","\n","turbulence_index=norm_data['turbulence_%s'%item].index\n","\n","for i in turbulence_index:\n","    for j in range(120):\n","        turbulence_data[j].append(turbulence_all[i-j])\n","        \n","df=pd.DataFrame()\n","for j in range(120):\n","    df['speed_%s'%j]=speed_data[j]\n","    \n","df_power=pd.DataFrame()\n","for j in range(120):\n","    df_power['power_%s'%j]=power_data[j]\n","\n","df_tem=pd.DataFrame()\n","for j in range(120):\n","    df_tem['tem_%s'%j]=tem_data[j]\n","\n","df_angle=pd.DataFrame()\n","for j in range(120):\n","    df_angle['angle_%s'%j]=angle_data[j]\n","\n","df_turbulence=pd.DataFrame()\n","for j in range(120):\n","    df_turbulence['turbulence_%s'%j]=turbulence_data[j]\n","\n","speed_df=pd.DataFrame()\n","speed_df['speed0']=df['speed_0']\n","speed_df['power1']=df_power['power_1']\n","speed_df['tem_0']=df_tem['tem_0']\n","speed_df['angle_0']=df_angle['angle_0']\n","speed_df['turbulence_0']=df_turbulence['turbulence_0']\n","\n","speed_df['speed1']=df['speed_1']\n","speed_df['power2']=df_power['power_2']\n","speed_df['tem_1']=df_tem['tem_1']\n","speed_df['angle_1']=df_angle['angle_1']\n","speed_df['turbulence_1']=df_turbulence['turbulence_1']\n","\n","speed_df['speed2']=df['speed_2']\n","speed_df['power3']=df_power['power_3']\n","speed_df['tem_2']=df_tem['tem_2']\n","speed_df['angle_2']=df_angle['angle_2']\n","speed_df['turbulence_2']=df_turbulence['turbulence_2']\n","\n","speed_df['speed3']=df['speed_3']\n","speed_df['power4']=df_power['power_4']\n","speed_df['tem_3']=df_tem['tem_3']\n","speed_df['angle_3']=df_angle['angle_3']\n","speed_df['turbulence_3']=df_turbulence['turbulence_3']\n","\n","speed_df['turbulence_4']=df_turbulence['turbulence_4']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2LbeJigZpje"},"outputs":[],"source":["speed_df=pd.DataFrame()\n","power=(power-np.min(power))/(np.max(power)-np.min(power))\n","\n","for i in range(4):\n","  speed_df['power%s'%(i+1)]=df_power['power_%s'%(i+1)]\n","  speed_df['speed%s'%i]=df['speed_%s'%i]\n","  speed_df['angle_%s'%i]=df_angle['angle_%s'%i]\n","  speed_df['tem_%s'%i]=df_tem['tem_%s'%i]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lN8K5L0aapgA"},"outputs":[],"source":["#Skewness and kurtosis analysis\n","col_name=speed_df.columns\n","for item in col_name:\n","    tmp=np.array(speed_df[item])\n","    tmp_1=(tmp-np.mean(tmp))/np.std(tmp)\n","    speed_df[item]=tmp_1\n","    \n","speed=np.array(speed_df)\n","\n","X_train, X_test, y_train, y_test = train_test_split(speed,power, test_size=0.2,)\n","X_train_1=X_train\n","X_test_1=X_test\n","X_train = np.reshape(X_train, (X_train.shape[0], 4, 4))\n","X_test = np.reshape(X_test, (X_test.shape[0], 4, 4))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNEbGEGUhrvx"},"outputs":[],"source":["#LossHistory class\n","class LossHistory(tf.keras.callbacks.Callback):\n","    def on_train_begin(self, logs={}):\n","        self.losses = {'batch':[], 'epoch':[]}\n","        self.accuracy = {'batch':[], 'epoch':[]}\n","        self.val_loss = {'batch':[], 'epoch':[]}\n","        self.val_acc = {'batch':[], 'epoch':[]}\n","\n","    def on_batch_end(self, batch, logs={}):\n","        self.losses['batch'].append(logs.get('loss'))\n","        self.accuracy['batch'].append(logs.get('acc'))\n","        self.val_loss['batch'].append(logs.get('val_loss'))\n","        self.val_acc['batch'].append(logs.get('val_acc'))\n","\n","    def on_epoch_end(self, batch, logs={}):\n","        self.losses['epoch'].append(logs.get('loss'))\n","        self.accuracy['epoch'].append(logs.get('acc'))\n","        self.val_loss['epoch'].append(logs.get('val_loss'))\n","        self.val_acc['epoch'].append(logs.get('val_acc'))\n","\n","    def loss_plot(self, loss_type):\n","        iters = range(len(self.losses[loss_type]))\n","        plt.figure()\n","        # # acc\n","        # plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n","        # loss\n","        plt.plot(iters, self.losses[loss_type], 'dodgerblue', label='train loss')\n","        if loss_type == 'epoch':\n","            # # val_acc\n","            # plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n","            # val_loss\n","            plt.plot(iters, self.val_loss[loss_type], 'darkorange', label='val loss')\n","        plt.grid(True)\n","        plt.xlabel(loss_type)\n","        plt.ylabel('loss')\n","        plt.legend(loc=\"upper right\")\n","        plt.savefig('loss{0}'.format(loss_type))\n","\n","    def acc_plot(self, loss_type):\n","        iters = range(len(self.losses[loss_type]))\n","        plt.figure()\n","        # acc\n","        plt.plot(iters, self.accuracy[loss_type], 'dodgerblue', label='train acc')\n","        # loss\n","        # plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n","        if loss_type == 'epoch':\n","            # val_acc\n","            plt.plot(iters, self.val_acc[loss_type], 'darkorange', label='val acc')\n","            # val_loss\n","            # plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n","        plt.grid(True)\n","        plt.xlabel(loss_type)\n","        plt.ylabel('acc')\n","        plt.legend(loc=\"upper right\")\n","        plt.savefig('acc{0}'.format(loss_type))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJx7U51SiwMI"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import time\n","import math\n","import tensorflow as tf\n","from keras import Model\n","from keras.models import Sequential\n","import keras.backend as K\n","from keras.layers import *\n","from keras.models import load_model\n","from keras.utils.vis_utils import plot_model\n","\n","from keras import regularizers\n","from keras.constraints import non_neg\n","from keras.initializers import RandomUniform,RandomNormal\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hNLg4DvjZD1"},"outputs":[],"source":["TIME_STEPS=4\n","INPUT_DIM=4\n","SINGLE_ATTENTION_VECTOR=False\n","\n","def attention_3d_block(inputs):\n","    # inputs.shape = (batch_size, time_steps, input_dim)\n","    input_dim = int(inputs.shape[2])\n","    print(input_dim)\n","    a = Permute((2, 1))(inputs)\n","    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n","    a = Dense(TIME_STEPS, activation='softmax')(a)\n","    if SINGLE_ATTENTION_VECTOR:\n","        a = Lambda(lambda x: K.mean(x, axis=1), name='attention')(a)\n","        a = RepeatVector(input_dim)(a)\n","    a_probs = Permute((2, 1), name='attention_vec')(a)\n","    output_attention_mul = Multiply()([inputs, a_probs])\n","    return output_attention_mul\n","\n","def get_layer_output(model,layer_name,inputs):\n","    layer = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n","    layer_out = layer.predict(inputs)\n","    return np.mean(layer_out,axis=0)\n","\n","def create_lstm_attention_model():\n","  mean_inputs = tf.keras.Input((TIME_STEPS,INPUT_DIM))\n","  attention_mul = attention_3d_block(mean_inputs)\n","  mean_x = LSTM(64, return_sequences=True)(attention_mul)\n","  mean_x=LSTM(64, return_sequences=False)(mean_x)\n","  mean_x = Dense(32,)(mean_x)\n","  mean_x=Dense(1)(mean_x)\n","  model = keras.Model(inputs=mean_inputs, outputs=mean_x)\n","  return model\n","\n","def create_lstm_attention_model():\n","  mean_inputs = tf.keras.Input((TIME_STEPS,INPUT_DIM))\n","  attention_mul = attention_3d_block(mean_inputs)\n","  mean_x = LSTM(128, return_sequences=True)(attention_mul)\n","  mean_x=LSTM(64, return_sequences=False)(mean_x)\n","  mean_x = Dense(64,)(mean_x)\n","  mean_x=Dense(1)(mean_x)\n","  model = keras.Model(inputs=mean_inputs, outputs=mean_x)\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPG0AvK0jsbU"},"outputs":[],"source":["num_epochs = 50\n","mean_model = create_lstm_attention_model()\n","\n","speed=np.array(speed_df)\n","X_train, X_test, y_train, y_test = train_test_split(speed,power, test_size=0.2,)\n","X_train_1=X_train\n","X_test_1=X_test\n","X_train = np.reshape(X_train, (X_train.shape[0], 4, 4))\n","X_test = np.reshape(X_test, (X_test.shape[0], 4, 4))\n","\n","history = LossHistory()\n","mean_model.compile(optimizer='adam',loss='mean_squared_error',metrics=[keras.metrics.RootMeanSquaredError()])#loss,learning_rate=learning_rate;keras.optimizers.RMSprop()\n","\n","mean_model.fit(X_train,y_train,batch_size=64, epochs=num_epochs,validation_split=0.15,callbacks=[history])    "]},{"cell_type":"markdown","metadata":{"id":"T0hU-s0ql3qe"},"source":["### Whether using historical data to analyze"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1650789850944,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"vBChH3QnvYUv","outputId":"3e353588-7ff6-4797-9c44-a423fe16f80d"},"outputs":[{"data":{"text/plain":["(24148,)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["np.shape(y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vlkFzFtZl7w-"},"outputs":[],"source":["%%time\n","\n","TIME_STEPS=4\n","INPUT_DIM=4\n","SINGLE_ATTENTION_VECTOR=False\n","\n","def attention_3d_block(inputs):\n","    # inputs.shape = (batch_size, time_steps, input_dim)\n","    input_dim = int(inputs.shape[2])\n","    print(input_dim)\n","    a = Permute((2, 1))(inputs)\n","    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n","    a = Dense(TIME_STEPS, activation='softmax')(a)\n","    if SINGLE_ATTENTION_VECTOR:\n","        a = Lambda(lambda x: K.mean(x, axis=1), name='attention')(a)\n","        a = RepeatVector(input_dim)(a)\n","    a_probs = Permute((2, 1), name='attention_vec')(a)\n","    output_attention_mul = Multiply()([inputs, a_probs])\n","    return output_attention_mul\n","\n","def create_lstm_attention_model_notime():\n","  mean_inputs = tf.keras.Input((1,3))\n","  mean_x = LSTM(64, return_sequences=True)(mean_inputs)\n","  mean_x=LSTM(64, return_sequences=False)(mean_x)\n","  mean_x = Dense(32,)(mean_x)\n","  mean_x=Dense(1)(mean_x)\n","  model = keras.Model(inputs=mean_inputs, outputs=mean_x)\n","  return model\n","\n","def create_lstm_attention_model_withtime():\n","  mean_inputs = tf.keras.Input((TIME_STEPS,INPUT_DIM))\n","  mean_x = LSTM(64, return_sequences=True)(mean_inputs)\n","  mean_x=LSTM(64, return_sequences=False)(mean_x)\n","  mean_x = Dense(32,)(mean_x)\n","  mean_x=Dense(1)(mean_x)\n","  model = keras.Model(inputs=mean_inputs, outputs=mean_x)\n","  return model\n","\n","def create_lstm_attention_model_attention():\n","  mean_inputs = tf.keras.Input((TIME_STEPS,INPUT_DIM))\n","  attention_mul = attention_3d_block(mean_inputs)\n","  mean_x = LSTM(64, return_sequences=True)(attention_mul)\n","  mean_x=LSTM(64, return_sequences=False)(mean_x)\n","  mean_x = Dense(32,)(mean_x)\n","  mean_x=Dense(1)(mean_x)\n","  model = keras.Model(inputs=mean_inputs, outputs=mean_x)\n","  return model\n","  \n","X_train, X_test, y_train, y_test = train_test_split(speed_df,power, test_size=0.2,)\n","X_train_2, X_test_2 = X_train[['speed0','angle_0','tem_0']],X_test[['speed0','angle_0','tem_0']]\n","\n","\n","model1=create_lstm_attention_model_withtime()\n","model2=create_lstm_attention_model_notime()\n","model3=create_lstm_attention_model_attention()\n","\n","X_train=np.array(X_train)\n","X_test=np.array(X_test)\n","\n","X_train = np.reshape(X_train, (X_train.shape[0], 4, 4))\n","X_test = np.reshape(X_test, (X_test.shape[0], 4, 4))\n","\n","X_train_2=np.array(X_train_2)\n","X_test_2=np.array(X_test_2)\n","\n","X_train_2 = np.reshape(X_train_2, (X_train_2.shape[0], 1, 3))\n","X_test_2 = np.reshape(X_test_2, (X_test_2.shape[0], 1, 3))\n","\n","\n","history3 = LossHistory()\n","model3.compile(optimizer='adam',loss='mean_squared_error',metrics=[keras.metrics.RootMeanSquaredError()])#loss,learning_rate=learning_rate;keras.optimizers.RMSprop()\n","model3.fit(X_train,y_train,batch_size=64, epochs=50,validation_split=0.15,callbacks=[history3])\n","\n","history1 = LossHistory()\n","model1.compile(optimizer='adam',loss='mean_squared_error',metrics=[keras.metrics.RootMeanSquaredError()])#loss,learning_rate=learning_rate;keras.optimizers.RMSprop()\n","model1.fit(X_train,y_train,batch_size=64, epochs=50,validation_split=0.15,callbacks=[history1])\n","\n","history2 = LossHistory()\n","model2.compile(optimizer='adam',loss='mean_squared_error',metrics=[keras.metrics.RootMeanSquaredError()])#loss,learning_rate=learning_rate;keras.optimizers.RMSprop()\n","model2.fit(X_train_2,y_train,batch_size=64, epochs=50,validation_split=0.15,callbacks=[history1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":646,"status":"ok","timestamp":1650791042592,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"APbi2x5Ajz-3","outputId":"54dd5c58-f62d-4d61-ca80-73ad5db6ca0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["rmse:0.04351730761019866\n","mae:0.02919223896158769\n","r2:0.9748938355682032\n","mape:0.14064352920746015\n"]}],"source":["from sklearn.metrics import mean_squared_error as MSE,mean_absolute_error as MAE,max_error as ME,r2_score \n","from sklearn import metrics\n","\n","#preds = mean_model.predict(X_test)\n","prediction_mean = model2(X_test_2)\n","\n","r_2=r2_score(prediction_mean,y_test)\n","rmse = np.sqrt(MSE(prediction_mean, y_test))\n","mae = MAE(prediction_mean, y_test)\n","mape_ = metrics.mean_absolute_percentage_error(prediction_mean, y_test)\n","mse=MSE(prediction_mean, y_test)\n","\n","print('rmse:%s'%rmse)\n","print('mae:%s'%mae)\n","print('r2:%s'%r_2)\n","print('mape:%s'%mape_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHZ8cqVMpg6u"},"outputs":[],"source":["att = get_layer_output(mean_model,'attention_vec',X_test)\n","print(att.flatten())\n","x = np.array(speed_df.columns)\n","\n","sns.barplot(att.flatten(),x)\n","print(np.sum(att))"]},{"cell_type":"markdown","metadata":{"id":"_07w7i9JiLAX"},"source":["### Compared Experiment"]},{"cell_type":"markdown","metadata":{"id":"ES0mC9xPlu1K"},"source":["XGBOOST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iZ9e3EvriMXU"},"outputs":[],"source":["from xgboost.sklearn import XGBRegressor\n","from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n","from sklearn.linear_model import Lasso, LassoCV, LinearRegression\n","from sklearn.metrics import mean_squared_error as MSE,mean_absolute_error as MAE,max_error as ME,r2_score \n","from sklearn import metrics\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.neighbors import KNeighborsRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fppBlJpClnoy"},"outputs":[],"source":["xgboost = XGBRegressor(gamma=0,                 \n","                 learning_rate=0.01,\n","                 max_depth=5,\n","                 min_child_weight=3,\n","                 n_estimators=300,                                                                    \n","                 reg_alpha=0,\n","                 reg_lambda=0,\n","                 subsample=0.8,\n","                 seed=42) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11346,"status":"ok","timestamp":1649042713538,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"sqIWSOEpl4Gm","outputId":"896b4e86-2868-4ac3-de8a-45b459659969"},"outputs":[{"name":"stdout","output_type":"stream","text":["[03:25:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"]}],"source":["xgboost.fit(X_train_1,y_train)\n","preds= xgboost.predict(X_test_1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1060,"status":"ok","timestamp":1649044054089,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"olcQQ3QZl8jh","outputId":"9ceda8d0-7d4e-459e-f6b9-2fd454b2b102"},"outputs":[{"name":"stdout","output_type":"stream","text":["rmse:0.051882507513318896\n","mae:0.035507519201023525\n","r2:0.9646321406362895\n","mape:0.2028401696389018\n"]}],"source":["#preds = mean_model.predict(X_test)\n","prediction_mean = mean_model(X_test)\n","\n","r_2=r2_score(preds,y_test)\n","rmse = np.sqrt(MSE(preds, y_test))\n","mae = MAE(preds, y_test)\n","mape_ = metrics.mean_absolute_percentage_error(preds, y_test)\n","\n","print('rmse:%s'%rmse)\n","print('mae:%s'%mae)\n","print('r2:%s'%r_2)\n","print('mape:%s'%mape_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRvnIrzunEJD"},"outputs":[],"source":["Rforest = RandomForestRegressor(n_estimators=50, criterion='mse', max_depth=100, min_samples_split=2,\n","                                max_features='auto',)\n","\n","Rforest.fit(X_train_1, y_train)\n","preds= Rforest.predict(X_test_1) "]},{"cell_type":"markdown","metadata":{"id":"N3WgBAfBn52I"},"source":["cubic polynomial"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75FTx_Vtn7iu"},"outputs":[],"source":["def polyfit(degree, x_train, y, x_test):\n","    poly_features = PolynomialFeatures(degree=degree)\n","    \n","    X_train_poly = poly_features.fit_transform(x_train)\n","    poly_model = LinearRegression()\n","    lm_poly = poly_model.fit(X_train_poly, y)\n","    \n","    R_squared = lm_poly.score(X_train_poly, y)\n","    predictions = lm_poly.predict(poly_features.fit_transform(x_test))\n","    \n","    return R_squared, predictions\n","\n","r2, preds = polyfit(3, X_train_1, y_train, X_test_1)"]},{"cell_type":"markdown","metadata":{"id":"0vcQ1RK1osLx"},"source":["KNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yb0rSzS9oYnT"},"outputs":[],"source":["model = KNeighborsRegressor(n_neighbors = 25,weights='uniform',metric = 'minkowski',metric_params={'p': 3})\n","model.fit(X_train_1, y_train)  #fit the model\n","preds=model.predict(X_test_1) #make prediction on test set"]},{"cell_type":"markdown","metadata":{"id":"vhLs5Lh0qW0d"},"source":["DNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnOYhKE5pKGa"},"outputs":[],"source":["from keras.models import Sequential, load_model\n","from keras.initializers import he_normal\n","from keras.layers import Dense, Dropout\n","from keras import regularizers\n","from keras.callbacks import EarlyStopping, ModelCheckpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"geo28MKBqYdn"},"outputs":[],"source":["def define_and_fit_model(layer_nodes, num_epochs, batch, optimiser, dropout_rate=0.1, input_nodes=16):\n","\n","    num_layers = len(layer_nodes)\n","    \n","    ### Define Model ###\n","    model = Sequential()\n","    first = True\n","    for l in range(num_layers):\n","        if first:\n","            # Input layer\n","            model.add(Dense(layer_nodes[l], input_dim=input_nodes,\n","                            kernel_initializer=he_normal(seed=1),\n","                            # kernel_regularizer=regularizers.l2(0.1),\n","                            activation='relu'))\n","            first = False\n","            \n","        else:\n","            # Hidden layers\n","            model.add(Dense(layer_nodes[l],\n","                            kernel_initializer=he_normal(seed=1),\n","                            # kernel_regularizer=regularizers.l2(0.1),\n","                            activation='relu'))\n","        if dropout_rate > 0:\n","            model.add(Dropout(dropout_rate))\n","            \n","    # Output layer\n","    model.add(Dense(1, kernel_initializer=he_normal(seed=1)))\n","    \n","    print(model.summary())\n","    \n","    ### Fit Model ###\n","    model.compile(loss='mean_squared_error', optimizer=optimiser)\n","    checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='val_loss', verbose=0,\n","                                 save_best_only=True, mode='min')\n","    callbacks_list = [checkpoint]\n","    history = model.fit(X_train_1, y_train, \n","                        epochs=num_epochs, batch_size=batch, verbose=0,\n","                        validation_split=0.15,\n","                        callbacks=callbacks_list)\n","\n","    run_codename = \"run__{layers}__b{batch_size}__{opt}__d{drop}__e{epochs}\".format(\n","        layers=\"_\".join(map(str, layer_nodes)), batch_size='%04d' % batch, opt=optimiser, \n","        drop=('%.4f' % 0.01).replace('.','_'), epochs= '%05d' % batch)\n","    \n","    return model, history, run_codename"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hvv6N1Dtsn8t"},"outputs":[],"source":["model, hist, name = define_and_fit_model(layer_nodes=[64], num_epochs=50, batch=64, optimiser='nadam')\n","preds = model.predict(X_test_1)"]},{"cell_type":"markdown","metadata":{"id":"FwNlzJcDsiZd"},"source":["Heteroscedasticity spline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJj0ke3HslO0"},"outputs":[],"source":["import numpy as np\n","import pymc3 as pm\n","import theano.tensor as tt\n","import matplotlib.pyplot as plt\n","import arviz as az\n","from scipy.interpolate import BSpline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TgA4cLHssnlx"},"outputs":[],"source":["speed_0=np.array(speed_df['speed0'])\n","speed_1=np.array(speed_df['speed1'])\n","power_1=np.array(speed_df['power1'])\n","power_2=np.array(speed_df['power2'])\n","power_0=np.array(norm_data['power(kW)_1'])#原始的power数据"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8ef7q_6tUH4"},"outputs":[],"source":["#多变量\n","NK = 12\n","knots_1 = np.linspace(speed_0[:10000].min(),speed_0[:10000].max(),NK)\n","knots_2 = np.linspace(power_1[:10000].min(),power_1[:10000].max(),NK)\n","# Univariate spline in the B-spline basis\n","basis_funcs_1 = BSpline(knots_1, np.eye(NK), k=3)  # cubic spline representation\n","basis_funcs_2 = BSpline(knots_2, np.eye(NK), k=3)  # cubic spline representation\n","Bx_1_1 = basis_funcs_1(speed_0[:10000])  # coefficients\n","Bx_1_2 = basis_funcs_1(speed_1[:10000])  # coefficients\n","Bx_2_1 = basis_funcs_2(power_1[:10000])  # coefficients\n","Bx_2_2 = basis_funcs_2(power_2[:10000])  # coefficients\n","Bx=np.c_[Bx_1_1,Bx_1_2,Bx_2_1,Bx_2_2]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"elapsed":11185005,"status":"ok","timestamp":1649055664114,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"gvIOOLwLtX2z","outputId":"361a5007-85e5-4e22-f2d5-895ad6b56978"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n","WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"]},{"data":{"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      <progress value='2476' class='' max='2476' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [2476/2476 00:21<00:00 logp = 16,140, ||grad|| = 91.335]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["Auto-assigning NUTS sampler...\n","Initializing NUTS using advi+adapt_diag...\n"]},{"data":{"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      <progress value='79916' class='' max='200000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      39.96% [79916/200000 01:06<01:40 Average Loss = -15,020]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Convergence achieved at 80100\n","Interrupted at 80,099 [40%]: Average Loss = 7,769.9\n","Sequential sampling (2 chains in 1 job)\n","NUTS: [sd, betai, sd0, mu0]\n"]},{"data":{"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      <progress value='7000' class='' max='7000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [7000/7000 1:30:42<00:00 Sampling chain 0, 0 divergences]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      <progress value='7000' class='' max='7000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [7000/7000 1:31:40<00:00 Sampling chain 1, 0 divergences]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Sampling 2 chains for 2_000 tune and 5_000 draw iterations (4_000 + 10_000 draws total) took 10944 seconds.\n","The acceptance probability does not match the target. It is 0.8808883038558398, but should be close to 0.8. Try to increase the number of tuning steps.\n","The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n","The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n","The estimated number of effective samples is smaller than 200 for some parameters.\n"]}],"source":["with pm.Model() as splines:\n","    # hyper-priors\n","    mu0 = pm.Normal('mu0', mu=0., sd=10.)\n","    sd0 = pm.HalfCauchy('sd0', beta=5.)\n","    # priors (must use the same NK for y1 & y2!)\n","    betai = pm.Normal('betai', mu=mu0, sd=sd0, shape=NK*4)\n","    sd = pm.HalfCauchy('sd', beta=10.)\n","    # link (spline regression)\n","    mu = pm.Deterministic('mu', pm.math.dot(Bx, betai))\n","    # likelihood\n","    yi = pm.Normal('yhat', mu=mu, sd=sd, observed=power[:10000])\n","    # MCMC\n","    start=pm.find_MAP()\n","    chain = pm.sample(draws=5000, tune=2000, chains=2,init = 'advi+adapt_diag',start=start)#,target_accept=0.95"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"buJvrJwEJSpP"},"outputs":[],"source":["betai_est = chain.get_values(varname='betai', burn=1000, thin=2, combine=True)\n","y_est = np.dot(Bx, betai_est.mean(axis=0))\n","plt.scatter(speed_0[:10000], power[:10000], c='red')\n","plt.scatter(speed_0[:10000], y_est, c='royalblue')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":534,"status":"ok","timestamp":1649059382809,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"vRla9xn8JpnR","outputId":"2b5d058e-243c-4407-834f-fda45e03bdec"},"outputs":[{"name":"stdout","output_type":"stream","text":["rmse:0.051497417911227424\n","mae:0.03542262070077466\n","r2:0.9672879995316568\n","mape:0.1346861475709001\n"]}],"source":["from sklearn.metrics import mean_squared_error as MSE,mean_absolute_error as MAE,max_error as ME,r2_score \n","from sklearn import metrics\n","\n","r_2=r2_score(y_est,power[10000:])\n","rmse = np.sqrt(MSE(y_est,power[10000:]))\n","mae = MAE(y_est,power[10000:])\n","mape_ = metrics.mean_absolute_percentage_error(y_est,power[10000:])\n","mse=MSE(y_est,power[10000:])\n","\n","print('rmse:%s'%rmse)\n","print('mae:%s'%mae)\n","print('r2:%s'%r_2)\n","print('mape:%s'%mape_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9i8VeRt_mN-A"},"outputs":[],"source":["Bx_1_1 = basis_funcs_1(speed_0[10000:])  # coefficients\n","Bx_1_2 = basis_funcs_1(speed_1[10000:])  # coefficients\n","Bx_2_1 = basis_funcs_2(power_1[10000:])  # coefficients\n","Bx_2_2 = basis_funcs_2(power_2[10000:])  # coefficients\n","Bx_test=np.c_[Bx_1_1,Bx_1_2,Bx_2_1,Bx_2_2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvCeFkaCmOko"},"outputs":[],"source":["y_est = np.dot(Bx_test, betai_est.mean(axis=0))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNVe/HgyqBxaHjbfK6kZIX9","collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
