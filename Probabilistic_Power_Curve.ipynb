{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vae9FMuIZObB"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import math\n","import numpy as np\n","import pandas as pd\n","import warnings\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import random\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.model_selection import KFold, cross_val_score, train_test_split\n","\n","import tensorflow_probability as tfp\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM\n","\n","from tensorflow.keras.layers import StringLookup\n","\n","#ignore warnings\n","warnings.filterwarnings(\"ignore\")\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eTlhuftDwMqT"},"outputs":[],"source":["#改配色方案\n","# brewer2mpl.get_map args: set name set type number of colors\n","import matplotlib as mpl\n","\n","#mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=colors)\n","mpl.rcParams['font.family'] = 'Times New Roman'\n","\n","plt.rcParams['savefig.dpi'] = 300\n","plt.rcParams['figure.dpi'] = 300"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":20832,"status":"ok","timestamp":1671444950981,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"pFLQaynUZXVX","outputId":"32f94235-a297-4fa1-a35c-312fda6f8a25"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n#进行u-sd标准化\\ncol_name=speed_df.columns\\nfor item in col_name:\\n    tmp=np.array(speed_df[item])\\n    tmp_1=(tmp-np.mean(tmp))/np.std(tmp)\\n    speed_df[item]=tmp_1\\n\\npower=(power-np.min(power))/(np.max(power)-np.min(power))\\n'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["data=pd.read_csv('data.csv')\n","item=3\n","\n","data['time']=pd.to_datetime(data['time'],format = '%Y-%m-%d %H:%M:%S')\n","\n","turbulence = np.zeros((len(data), 1))\n","\n","speed_for_turbulence = np.array(data['speed(m/s)_%s'%item])\n","\n","for i in range(5,len(data)-1):\n","  tmp = speed_for_turbulence[(i-5):(i+1)]\n","  turbulence[i][0] = np.std(tmp)/(np.mean(tmp)+0.001)\n","\n","turbulence[turbulence>1] = 1\n","data['turbulence_%s'%item]=turbulence\n","\n","angle=np.array(data['angle(°)_%s'%item])\n","cos_angle=[np.cos(np.pi*(i/180)) for i in angle]\n","data['angle(°)_%s'%item]=cos_angle\n","\n","norm_data_original=data[data['state_%s'%item]==10]\n","norm_data1=norm_data_original[norm_data_original['active_value_%s'%item]==100]\n","norm_data2=norm_data_original[norm_data_original['active_value_%s'%item]==0]\n","\n","norm_data=pd.concat([norm_data1,norm_data2])\n","\n","ab_data=data[data['state_%s'%item]!=10]\n","limit_data=pd.concat([norm_data_original,norm_data],axis =0).drop_duplicates(keep=False)\n","limit_data=pd.concat([limit_data,ab_data])\n","\n","speed_all=np.array(data['speed(m/s)_%s'%item])\n","power_all=np.array(data['power(kW)_%s'%item])\n","temperature_all=np.array(data['temperature(℃)_%s'%item])\n","angle_all=np.array(data['angle(°)_%s'%item])\n","turbulence_all=np.array(data['turbulence_%s'%item])\n","\n","\n","speed=np.array(norm_data['speed(m/s)_%s'%item])\n","power=np.array(norm_data['power(kW)_%s'%item])\n","temperature=np.array(norm_data['temperature(℃)_%s'%item])\n","angle=np.array(norm_data['angle(°)_%s'%item])\n","turbulence=np.array(norm_data['turbulence_%s'%item])\n","\n","speed_index=norm_data['speed(m/s)_%s'%item].index \n","\n","#generate dataset\n","speed_data=[[] for i in range(120)]\n","power_data=[[] for i in range(120)]\n","tem_data=[[] for i in range(120)]\n","angle_data=[[] for i in range(120)]\n","turbulence_data=[[] for i in range(120)]\n","\n","speed_index=norm_data['speed(m/s)_%s'%item].index\n","for i in speed_index:\n","    for j in range(120):\n","        speed_data[j].append(speed_all[i-j])\n","\n","power_index=norm_data['power(kW)_%s'%item].index\n","\n","for i in power_index:\n","    for j in range(120):\n","        power_data[j].append(power_all[i-j])\n","    \n","tem_index=norm_data['temperature(℃)_%s'%item].index\n","\n","for i in tem_index:\n","    for j in range(120):\n","        tem_data[j].append(temperature_all[i-j])\n","\n","angle_index=norm_data['angle(°)_%s'%item].index\n","\n","for i in angle_index:\n","    for j in range(120):\n","        angle_data[j].append(angle_all[i-j])\n","\n","turbulence_index=norm_data['turbulence_%s'%item].index\n","\n","for i in turbulence_index:\n","    for j in range(120):\n","        turbulence_data[j].append(turbulence_all[i-j])\n","        \n","df=pd.DataFrame()\n","for j in range(120):\n","    df['speed_%s'%j]=speed_data[j]\n","    \n","df_power=pd.DataFrame()\n","for j in range(120):\n","    df_power['power_%s'%j]=power_data[j]\n","\n","df_tem=pd.DataFrame()\n","for j in range(120):\n","    df_tem['tem_%s'%j]=tem_data[j]\n","\n","df_angle=pd.DataFrame()\n","for j in range(120):\n","    df_angle['angle_%s'%j]=angle_data[j]\n","\n","df_turbulence=pd.DataFrame()\n","for j in range(120):\n","    df_turbulence['turbulence_%s'%j]=turbulence_data[j]\n","\n","speed_df=pd.DataFrame()\n","speed_df['speed0']=df['speed_0']\n","speed_df['power1']=df_power['power_1']\n","speed_df['tem_0']=df_tem['tem_0']\n","speed_df['angle_0']=df_angle['angle_0']\n","speed_df['turbulence_0']=df_turbulence['turbulence_0']\n","\n","speed_df['speed1']=df['speed_1']\n","speed_df['power2']=df_power['power_2']\n","speed_df['tem_1']=df_tem['tem_1']\n","speed_df['angle_1']=df_angle['angle_1']\n","speed_df['turbulence_1']=df_turbulence['turbulence_1']\n","\n","speed_df['speed2']=df['speed_2']\n","speed_df['power3']=df_power['power_3']\n","speed_df['tem_2']=df_tem['tem_2']\n","speed_df['angle_2']=df_angle['angle_2']\n","speed_df['turbulence_2']=df_turbulence['turbulence_2']\n","\n","speed_df['speed3']=df['speed_3']\n","speed_df['power4']=df_power['power_4']\n","speed_df['tem_3']=df_tem['tem_3']\n","speed_df['angle_3']=df_angle['angle_3']\n","speed_df['turbulence_3']=df_turbulence['turbulence_3']\n","\n","speed_df['turbulence_4']=df_turbulence['turbulence_4']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2LbeJigZpje"},"outputs":[],"source":["speed_df=pd.DataFrame()\n","power=(power-np.min(power))/(np.max(power)-np.min(power))\n","\n","for i in range(4):\n","  speed_df['power_%s'%(i+1)]=df_power['power_%s'%(i+1)]\n","  speed_df['speed_%s'%i]=df['speed_%s'%i]\n","  speed_df['angle_%s'%i]=df_angle['angle_%s'%i]\n","  speed_df['tem_%s'%i]=df_tem['tem_%s'%i]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lN8K5L0aapgA"},"outputs":[],"source":["# u-sd standardization\n","col_name=speed_df.columns\n","for item in col_name:\n","    tmp=np.array(speed_df[item])\n","    tmp_1=(tmp-np.mean(tmp))/np.std(tmp)\n","    speed_df[item]=tmp_1\n","\n","speed=np.array(speed_df)\n","\n","X_train, X_test, y_train, y_test = train_test_split(speed,power, test_size=0.2,)\n","X_train_1=X_train\n","X_test_1=X_test\n","X_train = np.reshape(X_train, (X_train.shape[0], 4, 4))\n","X_test = np.reshape(X_test, (X_test.shape[0], 4, 4))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNEbGEGUhrvx"},"outputs":[],"source":["#LossHistory class\n","class LossHistory(tf.keras.callbacks.Callback):\n","    def on_train_begin(self, logs={}):\n","        self.losses = {'batch':[], 'epoch':[]}\n","        self.accuracy = {'batch':[], 'epoch':[]}\n","        self.val_loss = {'batch':[], 'epoch':[]}\n","        self.val_acc = {'batch':[], 'epoch':[]}\n","\n","    def on_batch_end(self, batch, logs={}):\n","        self.losses['batch'].append(logs.get('loss'))\n","        self.accuracy['batch'].append(logs.get('acc'))\n","        self.val_loss['batch'].append(logs.get('val_loss'))\n","        self.val_acc['batch'].append(logs.get('val_acc'))\n","\n","    def on_epoch_end(self, batch, logs={}):\n","        self.losses['epoch'].append(logs.get('loss'))\n","        self.accuracy['epoch'].append(logs.get('acc'))\n","        self.val_loss['epoch'].append(logs.get('val_loss'))\n","        self.val_acc['epoch'].append(logs.get('val_acc'))\n","\n","    def loss_plot(self, loss_type):\n","        iters = range(len(self.losses[loss_type]))\n","        plt.figure()\n","        # # acc\n","        # plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n","        # loss\n","        plt.plot(iters, self.losses[loss_type], 'dodgerblue', label='train loss')\n","        if loss_type == 'epoch':\n","            # # val_acc\n","            # plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n","            # val_loss\n","            plt.plot(iters, self.val_loss[loss_type], 'darkorange', label='val loss')\n","        plt.grid(True)\n","        plt.xlabel(loss_type)\n","        plt.ylabel('loss')\n","        plt.legend(loc=\"upper right\")\n","        plt.savefig('loss{0}'.format(loss_type))\n","\n","    def acc_plot(self, loss_type):\n","        iters = range(len(self.losses[loss_type]))\n","        plt.figure()\n","        # acc\n","        plt.plot(iters, self.accuracy[loss_type], 'dodgerblue', label='train acc')\n","        # loss\n","        # plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n","        if loss_type == 'epoch':\n","            # val_acc\n","            plt.plot(iters, self.val_acc[loss_type], 'darkorange', label='val acc')\n","            # val_loss\n","            # plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n","        plt.grid(True)\n","        plt.xlabel(loss_type)\n","        plt.ylabel('acc')\n","        plt.legend(loc=\"upper right\")\n","        plt.savefig('acc{0}'.format(loss_type))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJx7U51SiwMI"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import time\n","import math\n","import tensorflow as tf\n","from keras import Model\n","from keras.models import Sequential\n","import keras.backend as K\n","from keras.layers import *\n","from keras.models import load_model\n","from keras.utils.vis_utils import plot_model\n","\n","from keras import regularizers\n","from keras.constraints import non_neg\n","from keras.initializers import RandomUniform,RandomNormal\n","from matplotlib import pyplot as plt\n","\n","tfd = tfp.distributions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hNLg4DvjZD1"},"outputs":[],"source":["TIME_STEPS=4\n","INPUT_DIM=4\n","SINGLE_ATTENTION_VECTOR=False\n","\n","def attention_3d_block(inputs):\n","    # inputs.shape = (batch_size, time_steps, input_dim)\n","    input_dim = int(inputs.shape[2])\n","    print(input_dim)\n","    a = Permute((2, 1))(inputs)\n","    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n","    a = Dense(TIME_STEPS, activation='softmax')(a)\n","    if SINGLE_ATTENTION_VECTOR:\n","        a = Lambda(lambda x: K.mean(x, axis=1), name='attention')(a)\n","        a = RepeatVector(input_dim)(a)\n","    a_probs = Permute((2, 1), name='attention_vec')(a)\n","    output_attention_mul = Multiply()([inputs, a_probs])\n","    return output_attention_mul\n","\n","def get_layer_output(model,layer_name,inputs):\n","    layer = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n","    layer_out = layer.predict(inputs)\n","    return np.mean(layer_out,axis=0)\n","\n","\n","def create_probablistic_attention_lstm_model():\n","  mean_inputs = tf.keras.Input((4,4))\n","  #attention_mul = attention_3d_block(mean_inputs)\n","  mean_x = LSTM(128, return_sequences=True,trainable=True)(mean_inputs)\n","  mean_x=LSTM(64, return_sequences=False,trainable=True)(mean_x)\n","  mean_x = Dense(64,trainable=True)(mean_x)\n","  mean_x = Dense(32,trainable=True)(mean_x)\n","  mean_x=Dense(1,trainable=True)(mean_x)\n","\n","  sigma_x =LSTM(128, return_sequences=True,trainable=False)(mean_inputs)\n","  sigma_x=LSTM(64, return_sequences=False,trainable=False)(sigma_x)\n","  sigma_x = Dense(32,trainable=False)(sigma_x)\n","  sigma_x=Dense(1,trainable=False)(sigma_x)\n","\n","  x = tf.concat([mean_x, sigma_x],1)\n","  # x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n","  # x = layers.Dense(2, activation='swish', kernel_regularizer=\"l2\")(x)\n","  outputs=tfp.layers.IndependentNormal(1)(x)\n","\n","  model = keras.Model(inputs=mean_inputs, outputs=outputs)\n","  return model\n","\n","def create_probablistic_attention_lstm_model_mean():\n","  mean_inputs = tf.keras.Input((4,4))\n","  attention_mul = attention_3d_block(mean_inputs)\n","  mean_x = LSTM(128, return_sequences=True,trainable=True)(attention_mul)\n","  mean_x=LSTM(64, return_sequences=False,trainable=True)(mean_x)\n","  mean_x = Dense(64,trainable=True)(mean_x)\n","  mean_x = Dense(32,trainable=True)(mean_x)\n","  mean_x=Dense(1,trainable=True)(mean_x)\n","\n","  model = keras.Model(inputs=mean_inputs, outputs=mean_x)\n","  return model\n","\n","def create_probablistic_attention_lstm_model_variance():\n","  variance_inputs = tf.keras.Input((4,4),name='main_input')\n","  attention_mul = attention_3d_block(variance_inputs)\n","  variance_x = LSTM(128, return_sequences=True,)(attention_mul)\n","  variance_x=LSTM(64, return_sequences=False)(variance_x)\n","  variance_x = Dense(64,)(variance_x)\n","  variance_x = Dense(32)(variance_x)\n","  variance_x=Dense(1,)(variance_x)\n","\n","  point_inputs = tf.keras.Input((1,),name='point_predict_input')\n","  x = keras.layers.concatenate([point_inputs, variance_x])\n","\n","  outputs=tfp.layers.IndependentNormal(1)(x)\n","\n","  model = keras.Model(inputs=[variance_inputs,point_inputs], outputs=outputs)\n","  return model\n","\n","\n","def BNN():\n","  mean_inputs = tf.keras.Input((4,4))\n","  attention_mul = attention_3d_block(mean_inputs)\n","  mean_x = LSTM(64, return_sequences=True,)(attention_mul)\n","  mean_x=LSTM(32, return_sequences=False)(mean_x)\n","  mean_x = Dense(32,)(mean_x)\n","  mean_x = Dense(16,)(mean_x)\n","  #mean_x=tfp.layers.DenseVariational(units=32,make_prior_fn=prior,make_posterior_fn=posterior,activation=\"sigmoid\",)(mean_inputs)\n","  #mean_x=tfp.layers.DenseVariational(units=16,make_prior_fn=prior,make_posterior_fn=posterior,activation=\"sigmoid\",)(mean_x)\n","  mean_x=Dense(2)(mean_x)\n","  outputs = tfp.layers.IndependentNormal(1)(mean_x)\n","  model = keras.Model(inputs=mean_inputs, outputs=outputs)\n","  return model\n","\n","def create_probablistic_attention_lstm_model_myown():\n","  mean_inputs = tf.keras.Input((4,4))\n","  #attention_mul = attention_3d_block(mean_inputs)\n","  mean_x = LSTM(128, return_sequences=True,trainable=True)(mean_inputs)\n","  mean_x=LSTM(64, return_sequences=False,trainable=True)(mean_x)\n","  mean_x = Dense(64,trainable=True)(mean_x)\n","  mean_x = Dense(32,trainable=True)(mean_x)\n","  mean_x=Dense(1,trainable=True)(mean_x)\n","\n","  sigma_x =LSTM(128, return_sequences=True,trainable=False)(mean_inputs)\n","  sigma_x=LSTM(64, return_sequences=False,trainable=False)(sigma_x)\n","  sigma_x = Dense(32,trainable=False)(sigma_x)\n","  sigma_x=Dense(1,trainable=False)(sigma_x)\n","\n","  outputs =layers.Concatenate(axis=1)([mean_x, sigma_x])\n","\n","  model = keras.Model(inputs=mean_inputs, outputs=outputs)\n","  return model\n","\n","def create_probablistic_attention_lstm_model_2():\n","  mean_inputs = tf.keras.Input((4,4))\n","  #attention_mul = attention_3d_block(mean_inputs)\n","  mean_x = LSTM(128, return_sequences=True)(mean_inputs)\n","  mean_x=LSTM(64, return_sequences=False)(mean_x)\n","  mean_x = Dense(64)(mean_x)\n","  mean_x = Dense(64)(mean_x)\n","  mean_x=Dense(2)(mean_x)\n","\n","  outputs=tfp.layers.IndependentNormal(1)(mean_x)\n","\n","  model = keras.Model(inputs=mean_inputs, outputs=outputs)\n","  return model\n","\n","def BNN_studentT():\n","  mean_inputs = tf.keras.Input((4,4))\n","  attention_mul = attention_3d_block(mean_inputs)\n","  mean_x = LSTM(64, return_sequences=True,)(attention_mul)\n","  mean_x=LSTM(32, return_sequences=False)(mean_x)\n","  mean_x = Dense(32,)(mean_x)\n","  mean_x = Dense(16,)(mean_x)\n","\n","  mean_x=Dense(2)(mean_x)\n","  #outputs=tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[..., 0],scale=1e-3+tf.math.softplus(0.05 * t[..., 1])))(mean_x)\n","  outputs=tfp.layers.DistributionLambda(lambda t: tfd.StudentT(df=100, loc=t[..., 0], scale=1e-3+tf.math.softplus(0.05 * t[..., 1])))(mean_x)\n","  #outputs = tfp.layers.IndependentNormal(1)(mean_x)\n","  model = keras.Model(inputs=mean_inputs,outputs=outputs)\n","  return model\n","\n","def BNN_studentT():\n","  mean_inputs = tf.keras.Input((4,4))\n","  attention_mul = attention_3d_block(mean_inputs)\n","  mean_x = LSTM(64, return_sequences=True,)(attention_mul)\n","  mean_x=LSTM(32, return_sequences=False)(mean_x)\n","  mean_x = Dense(32,)(mean_x)\n","  mean_x = Dense(16,)(mean_x)\n","\n","  mean_x=Dense(2)(mean_x)\n","  #outputs=tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[..., 0],scale=1e-3+tf.math.softplus(0.05 * t[..., 1])))(mean_x)\n","  outputs=tfp.layers.DistributionLambda(lambda t: tfd.StudentT(df=100, loc=t[..., 0], scale=1e-3+tf.math.softplus(0.05 * t[..., 1])))(mean_x)\n","  #outputs = tfp.layers.IndependentNormal(1)(mean_x)\n","  model = keras.Model(inputs=mean_inputs,outputs=outputs)\n","  return model\n","\n","def BNN_gamma():\n","  mean_inputs = tf.keras.Input((4,4))\n","  attention_mul = attention_3d_block(mean_inputs)\n","  mean_x = LSTM(64, return_sequences=True,)(attention_mul)\n","  mean_x=LSTM(32, return_sequences=False)(mean_x)\n","  mean_x = Dense(32,)(mean_x)\n","  mean_x = Dense(16,)(mean_x)\n","\n","  mean_x=Dense(2)(mean_x)\n","  #outputs=tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[..., 0],scale=1e-3+tf.math.softplus(0.05 * t[..., 1])))(mean_x)\n","  outputs=tfp.layers.DistributionLambda(lambda t: tfd.Gamma(concentration=tf.abs(t[..., 0]),rate=tf.abs(t[..., 1])))(mean_x)\n","  #outputs = tfp.layers.IndependentNormal(1)(mean_x)\n","  model = keras.Model(inputs=mean_inputs,outputs=outputs)\n","  return model\n","\n","def BNN_laplace():\n","  mean_inputs = tf.keras.Input((4,4))\n","  attention_mul = attention_3d_block(mean_inputs)\n","  mean_x = LSTM(64, return_sequences=True,)(attention_mul)\n","  mean_x=LSTM(32, return_sequences=False)(mean_x)\n","  mean_x = Dense(32,)(mean_x)\n","  mean_x = Dense(16,)(mean_x)\n","\n","  mean_x=Dense(2)(mean_x)\n","  outputs=tfp.layers.DistributionLambda(lambda t: tfd.Laplace(loc=t[..., 0],scale=1e-3+tf.math.softplus(0.05 * t[..., 1])))(mean_x)\n","  model = keras.Model(inputs=mean_inputs,outputs=outputs)\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exyZGhi-6hjF"},"outputs":[],"source":["import keras.backend as K\n","import math as m\n","def negative_loglikelihood(targets,estimated_distribution):\n","  #0.5*K.log(tf.constant(2*m.pi,dtype=tf.float32))+0.5*K.log(preds[1]*preds[1])+((targets-preds[0])**2)/(2*preds[1]*preds[1])\n","  return -estimated_distribution.log_prob(targets)"]},{"cell_type":"markdown","metadata":{"id":"GVlMlNsIFscP"},"source":["### Train and Predict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gf1dK4YZ61ff"},"outputs":[],"source":["X_train = np.load('distribution_analysis/X_train.npy', allow_pickle=True)\n","X_test = np.load('distribution_analysis/X_test.npy', allow_pickle=True)\n","y_train = np.load('distribution_analysis/y_train.npy', allow_pickle=True)\n","y_test = np.load('distribution_analysis/y_test.npy', allow_pickle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPG0AvK0jsbU"},"outputs":[],"source":["model=BNN_gamma()\n","\n","learning_rate=0.001\n","model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),loss=negative_loglikelihood,metrics=[keras.metrics.RootMeanSquaredError()],)\n","model.fit(X_train,y_train,batch_size=64, epochs=100,validation_split=0.15)\n","\n","prediction_distribution = model(X_test)\n","prediction_mean = prediction_distribution.mean().numpy().tolist()\n","prediction_stdv = prediction_distribution.stddev().numpy()"]},{"cell_type":"markdown","metadata":{"id":"6huCYZqjmj5O"},"source":["Distribution type discussion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cg3jSU1Tm2nf"},"outputs":[],"source":["def find_nearest(a, a0):\n","  idx = np.abs(a - a0).argmin()\n","  return idx\n","\n","def get_quantile(distribution):\n","  raw=np.arange(0,1,0.001)\n","  cdf_list=[]\n","  for value in raw:\n","    cdf_list.append(distribution.cdf(value).numpy()[0])\n","\n","  quantile_list=[]\n","  for item in np.arange(0.1,1,0.1):\n","    idx=find_nearest(cdf_list,item)\n","    quantile_list.append(raw[idx])\n","\n","  return quantile_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_U_OlqgVM9n"},"outputs":[],"source":["from tqdm import tqdm\n","\n","quantile_prediction=[]\n","for i in tqdm(range(len(X_test))):\n","  prediction_distribution = model(X_test[i].reshape(1,4,4))\n","  quantile=get_quantile(prediction_distribution)\n","  quantile_prediction.append(quantile)"]},{"cell_type":"markdown","metadata":{"id":"iqbhZq0lF84y"},"source":["Result measure"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":484,"status":"ok","timestamp":1650340740147,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"Cp0W8TRGF8E2","outputId":"e9be68c8-ed91-4e8f-8aa2-4606e08ba862"},"outputs":[{"name":"stdout","output_type":"stream","text":["rmse:0.048610748466574434\n","mae:0.03260880448551605\n","r2:0.9688960098152606\n","mape:0.12762777495481495\n"]}],"source":["from sklearn.metrics import mean_squared_error as MSE,mean_absolute_error as MAE,max_error as ME,r2_score\n","from sklearn import metrics\n","\n","r_2=r2_score(prediction_mean,y_test)\n","rmse = np.sqrt(MSE(prediction_mean,y_test))\n","mae = MAE(prediction_mean,y_test)\n","mape_ = metrics.mean_absolute_percentage_error(prediction_mean,y_test)\n","mse=MSE(prediction_mean,y_test)\n","\n","print('rmse:%s'%rmse)\n","print('mae:%s'%mae)\n","print('r2:%s'%r_2)\n","print('mape:%s'%mape_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IvfZKiP8ZASO"},"outputs":[],"source":["#CRPS\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import scipy.stats as st\n","\n","raw_point=y_test\n","pred_point=prediction_mean\n","std_data=prediction_stdv\n","crps_sum=0\n","mcrps=[]\n","for j in range(len(raw_point)):\n","    crps_sum+=std_data[j]*(((raw_point[j]-pred_point[j])/std_data[j])*(2*st.norm.cdf((raw_point[j]-pred_point[j])/std_data[j],0,1)-1)+\n","                               2*st.norm.pdf((raw_point[j]-pred_point[j])/std_data[j],0,1)-1/np.sqrt(3.1415926))\n","mcrps.append(crps_sum/len(raw_point))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1650340756580,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"JNK1G2LYmFgZ","outputId":"b1ade593-365d-4a01-87d0-805f4668817a"},"outputs":[{"data":{"text/plain":["[0.0240446764900032]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["mcrps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MoSxj-ZWZBAz"},"outputs":[],"source":["#MSC\n","#msc\n","sc_sum=0\n","MSC=[]\n","for j in range(len(raw_point)):\n","    # 均值为pred_point，标准差为pred_residual的正态分布在累积概率为0.5时对应的函数值\n","    cdf_point=st.norm.ppf([0.10,0.20,0.30,0.40,0.5,0.60,0.70,0.80,0.90,], loc=pred_point[j], scale=std_data[j])\n","    tmp=raw_point[j]<=cdf_point\n","    tmp=tmp+0\n","    sc_sum+=np.sum((tmp-[0.10,0.20,0.30,0.40,0.5,0.60,0.70,0.80,0.90,])*(raw_point[j]-cdf_point))\n","MSC.append(sc_sum/len(raw_point))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1650340773374,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"qBfEgx9dZGuc","outputId":"e265cbf5-3cd5-4525-dc23-f0969002de0b"},"outputs":[{"data":{"text/plain":["[-0.11802319056750858]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["MSC"]},{"cell_type":"markdown","metadata":{"id":"k24-CvnRO5nd"},"source":["Attention score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"milc369cO7mA"},"outputs":[],"source":["att = get_layer_output(model,'attention_vec',X_test)\n","print(att.flatten())\n","x = np.array(speed_df.columns)\n","\n","sns.barplot(att.flatten(),x)\n","print(np.sum(att))"]},{"cell_type":"markdown","metadata":{"id":"u5V1GRk5A-qw"},"source":["## metric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKhzYVC3IOYy"},"outputs":[],"source":["pred_result=pd.read_csv('result/pa_lstm_result_offline.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AyJ78Q5S0Jn4"},"outputs":[],"source":["pred_result.head()"]},{"cell_type":"markdown","metadata":{"id":"NQmA5Am4RHMx"},"source":["sharpness and reliability"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBGjhn6TA5qu"},"outputs":[],"source":["z_value=[2.58,1.96,1.65,1.29,1.04,0.85,0.68,0.53,0.39,0.26,0.13]\n","confidence=[0.99,0.95,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1]\n","shapness=[[] for i in range(11)]\n","reliability=[[] for i in range(11)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ExRFWQ6Re5u"},"outputs":[],"source":["# intercal prediction interval\n","#shapness and reliability\n","raw_point=np.array(pred_result['raw_data'])\n","pred_point=np.array(pred_result['pred_mean'])\n","std_data=np.array(pred_result['pred_std'])\n","\n","for j in range(11):\n","  upper_point=pred_point+z_value[j]*np.array(std_data)\n","  lower_point=pred_point-z_value[j]*np.array(std_data)\n","  shapness[j].append(np.mean(upper_point-lower_point))\n","  reliability[j].append(np.sum((raw_point<=upper_point)&(raw_point>=lower_point))/len(raw_point)-confidence[j])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbGQz8BV0_fq"},"outputs":[],"source":["result_df=pd.DataFrame()\n","result_df['sharpness_pa_lstm']=np.array(shapness).flatten()\n","result_df['reliability_pa_lstm']=np.array(reliability).flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XGpfBApa2N-2"},"outputs":[],"source":["result_df.to_csv('result/sha_rel.csv',index=False)"]},{"cell_type":"markdown","metadata":{"id":"MY4AdM2CU53A"},"source":["skill score and crps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0RS-cZe2ayz"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import scipy.stats as st\n","\n","#mcrps\n","crps_sum=0\n","mcrps=[]\n","for j in range(len(raw_point)):\n","    # 均值为pred_point，标准差为pred_residual的正态分布在累积概率为0.5时对应的函数值\n","    crps_sum+=std_data[j]*(((raw_point[j]-pred_point[j])/std_data[j])*(2*st.norm.cdf((raw_point[j]-pred_point[j])/std_data[j],0,1)-1)+\n","                               2*st.norm.pdf((raw_point[j]-pred_point[j])/std_data[j],0,1)-1/np.sqrt(3.1415926))\n","mcrps.append(crps_sum/len(raw_point))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lM0Hqy2c3q9w"},"outputs":[],"source":["#MSC\n","#msc\n","sc_sum=0\n","MSC=[]\n","for j in range(len(raw_point)):\n","    # 均值为pred_point，标准差为pred_residual的正态分布在累积概率为0.5时对应的函数值\n","    cdf_point=st.norm.ppf([0.05,0.10,0.15,0.20,0.25,0.30,0.35,0.40,0.45,0.5,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95], loc=pred_point[j], scale=std_data[j])\n","    tmp=raw_point[j]<=cdf_point\n","    tmp=tmp+0\n","    sc_sum+=np.sum((tmp-[0.05,0.10,0.15,0.20,0.25,0.30,0.35,0.40,0.45,0.5,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95])*(raw_point[j]-cdf_point))\n","MSC.append(sc_sum/len(raw_point))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":650,"status":"ok","timestamp":1649483571855,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"3OkVOWJQ4czG","outputId":"7c70b317-d5dd-40c9-ef11-a7905dcd1713"},"outputs":[{"data":{"text/plain":["[-0.430385673175324]"]},"execution_count":116,"metadata":{},"output_type":"execute_result"}],"source":["MSC"]},{"cell_type":"markdown","metadata":{"id":"D2lp1MrZRg-e"},"source":["### Baseline"]},{"cell_type":"markdown","metadata":{"id":"ijDvIcYf8z1E"},"source":["PPC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H39ZfGXk9M0R"},"outputs":[],"source":["!pip install fitter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oyLX8gKzxN-x"},"outputs":[],"source":["from fitter import Fitter\n","\n","speed_df=pd.DataFrame()\n","power=(power-np.min(power))/(np.max(power)-np.min(power))\n","\n","speed_df['speed_0']=df['speed_0']\n","speed_df['power']=power\n","\n","#15m/s is the boundary\n","speed_gap=[round(2+0.1*i,2) for i in range(131)]\n","\n","mean_list=[]\n","std_list=[]\n","for item in speed_gap:\n","  fit_power=speed_df[(speed_df['speed_0']>=item)&(speed_df['speed_0']<(item+0.1))]['power']\n","  fit_power=np.array(fit_power)\n","  f = Fitter(fit_power, distributions=['norm',])\n","  f.fit()\n","  norm_mean=f.fitted_param['norm'][0]\n","  norm_std=f.fitted_param['norm'][1]\n","  mean_list.append(norm_mean)\n","  std_list.append(norm_std)\n","\n","norm_df=pd.DataFrame()\n","norm_df['speed_range']=speed_gap\n","norm_df['mean']=mean_list\n","norm_df['std']=std_list\n","\n","###The previous code generates the mean and variance for each speed\n","\n","###The latter code generates the mean and variance for each speed\n","\n","#norm_df.to_csv('ppc_offline.csv',index=False)\n","X_train, X_test, y_train, y_test = train_test_split(np.array(speed_df['speed_0']),np.array(speed_df['power']),test_size=0.2,)\n","\n","X_test_new=[]\n","for item in X_test:\n","  if item<=15:\n","    X_test_new.append(round(item,1))\n","  else:\n","    X_test_new.append(15)\n","\n","#forecast\n","pred_mean=[]\n","pred_std=[]\n","for i in range(len(X_test_new)):\n","  tmp_mean=norm_df[norm_df['speed_range']==X_test_new[i]]['mean'].values[0]\n","  tmp_std=norm_df[norm_df['speed_range']==X_test_new[i]]['std'].values[0]\n","  pred_mean.append(tmp_mean)\n","  pred_std.append(tmp_std)\n","\n","pred_mean=np.array(pred_mean)\n","pred_std=np.array(pred_std)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":569,"status":"ok","timestamp":1649568293094,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"_BAoDoGp6a4t","outputId":"4e4d1aa5-fbfb-41e4-c782-8c8a63fa22f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["rmse:0.048236169569808175\n","mae:0.03219477467365577\n","r2:0.9702071397766286\n","mape:0.1465063948012958\n"]}],"source":["from sklearn.metrics import mean_squared_error as MSE,mean_absolute_error as MAE,max_error as ME,r2_score\n","from sklearn import metrics\n","\n","r_2=r2_score(pred_mean,y_test)\n","rmse = np.sqrt(MSE(pred_mean,y_test))\n","mae = MAE(pred_mean,y_test)\n","mape_ = metrics.mean_absolute_percentage_error(pred_mean,y_test)\n","mse=MSE(pred_mean,y_test)\n","\n","print('rmse:%s'%rmse)\n","print('mae:%s'%mae)\n","print('r2:%s'%r_2)\n","print('mape:%s'%mape_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wPjWz2L7_FW"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import scipy.stats as st\n","\n","#mcrps\n","raw_point=y_test\n","pred_point=pred_mean\n","std_data=pred_std\n","crps_sum=0\n","mcrps=[]\n","for j in range(len(raw_point)):\n","    crps_sum+=std_data[j]*(((raw_point[j]-pred_point[j])/std_data[j])*(2*st.norm.cdf((raw_point[j]-pred_point[j])/std_data[j],0,1)-1)+\n","                               2*st.norm.pdf((raw_point[j]-pred_point[j])/std_data[j],0,1)-1/np.sqrt(3.1415926))\n","mcrps.append(crps_sum/len(raw_point))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N3BbNWkn8LMm"},"outputs":[],"source":["#MSC\n","#msc\n","sc_sum=0\n","MSC=[]\n","for j in range(len(raw_point)):\n","    cdf_point=st.norm.ppf([0.10,0.20,0.30,0.40,0.5,0.60,0.70,0.80,0.90,], loc=pred_point[j], scale=std_data[j])\n","    tmp=raw_point[j]<=cdf_point\n","    tmp=tmp+0\n","    sc_sum+=np.sum((tmp-[0.10,0.20,0.30,0.40,0.5,0.60,0.70,0.80,0.90,])*(raw_point[j]-cdf_point))\n","MSC.append(sc_sum/len(raw_point))"]},{"cell_type":"markdown","metadata":{"id":"no7RXsObw1uM"},"source":["KDE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9Xf4m8P8YM7"},"outputs":[],"source":["import numpy as np\n","\n","def get_kde(x,data_array,bandwidth):\n","    def gauss(x):\n","        import math\n","        return (1/math.sqrt(2*math.pi))*math.exp(-0.5*(x**2))\n","    N=len(data_array)\n","    res=0\n","    if len(data_array)==0:\n","        return 0\n","    for i in range(len(data_array)):\n","        res += gauss((x-data_array[i])/bandwidth)\n","    res /= (N*bandwidth)\n","    return res\n","\n","def kde_y(input_array,c):\n","    bandwidth=c*np.std(input_array)*(len(input_array)**(-1/5))\n","    x_array=np.linspace(min(input_array),max(input_array),100)\n","    y_array=[get_kde(x_array[i],input_array,bandwidth) for i in range(x_array.shape[0])]\n","    return x_array,y_array\n","\n","def quantile_ind(y_array,x_array,a):\n","    s1 = 0\n","    index = 0\n","    for i in y_array:\n","        s1+=i*(x_array[1]-x_array[0])\n","        index+=1\n","        if s1>a:\n","            break\n","    return index-1\n","\n","def interpolation(x_array,y_array,index,a):\n","\n","    d = x_array[1]-x_array[0]\n","    if index !=0:\n","       x = (a-np.sum(y_array[:index])*d)/(y_array[index]*d)*(x_array[index]-x_array[index-1])+x_array[index-1]\n","       return x\n","    else:\n","       #x = a/(y_array[index]*d)*x_array[index]\n","       x = x_array[0]\n","       return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHarSZ3Q0fpO"},"outputs":[],"source":["speed_df=pd.DataFrame()\n","power=(power-np.min(power))/(np.max(power)-np.min(power))\n","\n","speed_df['speed_0']=df['speed_0']\n","speed_df['power']=power\n","\n","#15m/s is the boundary\n","speed_gap=[round(2+0.1*i,2) for i in range(131)]\n","\n","\n","y10=[]\n","y20=[]\n","y30=[]\n","y40=[]\n","y50=[]\n","y60=[]\n","y70=[]\n","y80=[]\n","y90=[]\n","\n","ind10=[]\n","ind20=[]\n","ind30=[]\n","ind40=[]\n","ind50=[]\n","ind60=[]\n","ind70=[]\n","ind80=[]\n","ind90=[]\n","\n","for item in speed_gap:\n","  fit_power=speed_df[(speed_df['speed_0']>=item)&(speed_df['speed_0']<(item+0.1))]['power']\n","  fit_power=np.array(fit_power)\n","  x_array, y_array = kde_y(fit_power,c=1.06)\n","  in10 = quantile_ind(y_array,x_array,0.1)\n","  in20 = quantile_ind(y_array,x_array,0.2)\n","  in30 = quantile_ind(y_array,x_array,0.3)\n","  in40 = quantile_ind(y_array,x_array,0.4)\n","  in50 = quantile_ind(y_array,x_array,0.5)\n","  in60 = quantile_ind(y_array,x_array,0.6)\n","  in70 = quantile_ind(y_array,x_array,0.7)\n","  in80 = quantile_ind(y_array,x_array,0.8)\n","  in90 = quantile_ind(y_array,x_array,0.9)\n","\n","  ind10.append(in10)\n","  ind20.append(in20)\n","  ind30.append(in30)\n","  ind40.append(in40)\n","  ind50.append(in50)\n","  ind60.append(in60)\n","  ind70.append(in70)\n","  ind80.append(in80)\n","  ind90.append(in90)\n","  y10.append(interpolation(x_array,y_array,in10,0.1))\n","  y20.append(interpolation(x_array,y_array,in20,0.2))\n","  y30.append(interpolation(x_array,y_array,in30,0.3))\n","  y40.append(interpolation(x_array,y_array,in40,0.4))\n","  y50.append(interpolation(x_array,y_array,in50,0.5))\n","  y60.append(interpolation(x_array,y_array,in60,0.6))\n","  y70.append(interpolation(x_array,y_array,in70,0.7))\n","  y80.append(interpolation(x_array,y_array,in80,0.8))\n","  y90.append(interpolation(x_array,y_array,in90,0.9))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LuGMaE_CNgkv"},"outputs":[],"source":["norm_df=pd.DataFrame()\n","norm_df['speed_range']=speed_gap\n","norm_df['q10']=y10\n","norm_df['q20']=y20\n","norm_df['q30']=y30\n","norm_df['q40']=y40\n","norm_df['q50']=y50\n","norm_df['q60']=y60\n","norm_df['q70']=y70\n","norm_df['q80']=y80\n","norm_df['q90']=y90"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICxBttAIOjdy"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(np.array(speed_df['speed_0']),np.array(speed_df['power']),test_size=0.2,)\n","\n","X_test_new=[]\n","for item in X_test:\n","  if item<=15:\n","    X_test_new.append(round(item,1))\n","  else:\n","    X_test_new.append(15)\n","\n","#预测\n","y10=[]\n","y20=[]\n","y30=[]\n","y40=[]\n","y50=[]\n","y60=[]\n","y70=[]\n","y80=[]\n","y90=[]\n","\n","for i in range(len(X_test_new)):\n","  y10.append(norm_df[norm_df['speed_range']==X_test_new[i]]['q10'].values[0])\n","  y20.append(norm_df[norm_df['speed_range']==X_test_new[i]]['q20'].values[0])\n","  y30.append(norm_df[norm_df['speed_range']==X_test_new[i]]['q30'].values[0])\n","  y40.append(norm_df[norm_df['speed_range']==X_test_new[i]]['q40'].values[0])\n","  y50.append(norm_df[norm_df['speed_range']==X_test_new[i]]['q50'].values[0])\n","  y60.append(norm_df[norm_df['speed_range']==X_test_new[i]]['q60'].values[0])\n","  y70.append(norm_df[norm_df['speed_range']==X_test_new[i]]['q70'].values[0])\n","  y80.append(norm_df[norm_df['speed_range']==X_test_new[i]]['q80'].values[0])\n","  y90.append(norm_df[norm_df['speed_range']==X_test_new[i]]['q90'].values[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I7Zsd0C7Q969"},"outputs":[],"source":["pred_df=pd.DataFrame()\n","pred_df['q10']=y10\n","pred_df['q20']=y20\n","pred_df['q30']=y30\n","pred_df['q40']=y40\n","pred_df['q50']=y50\n","pred_df['q60']=y60\n","pred_df['q70']=y70\n","pred_df['q80']=y80\n","pred_df['q90']=y90\n","\n","pred_df['raw']=y_test\n","pred_df.to_csv('result/KDE_inshore.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":636,"status":"ok","timestamp":1649677858279,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"aP68YvDpSfwI","outputId":"8f7a7ba4-4108-4daa-ef11-9bcf837a1a68"},"outputs":[{"name":"stdout","output_type":"stream","text":["rmse:0.05501909424925353\n","mae:0.03746694257729789\n","r2:0.9607973362980033\n","mape:0.14529681322480595\n"]}],"source":["from sklearn.metrics import mean_squared_error as MSE,mean_absolute_error as MAE,max_error as ME,r2_score\n","from sklearn import metrics\n","\n","prediction_mean=np.array(y50)\n","r_2=r2_score(prediction_mean,y_test)\n","rmse = np.sqrt(MSE(prediction_mean,y_test))\n","mae = MAE(prediction_mean,y_test)\n","mape_ = metrics.mean_absolute_percentage_error(prediction_mean,y_test)\n","mse=MSE(prediction_mean,y_test)\n","\n","print('rmse:%s'%rmse)\n","print('mae:%s'%mae)\n","print('r2:%s'%r_2)\n","print('mape:%s'%mape_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzlhgUgTVaPz"},"outputs":[],"source":["#MSC\n","sc_sum=0\n","MSC=[]\n","raw_point=y_test\n","test_pred=np.array(pred_df[pred_df.columns[0:-1]])\n","for j in range(len(raw_point)):\n","    # Function value of the normal distribution with mean pred_point and standard deviation pred_residual when the cumulative probability is 0.5\n","    cdf_point=test_pred[j]\n","    tmp=raw_point[j]<=cdf_point\n","    tmp=tmp+0\n","    sc_sum+=np.sum((tmp-[0.10,0.20,0.30,0.40,0.5,0.60,0.70,0.80,0.90])*(raw_point[j]-cdf_point))\n","MSC.append(sc_sum/len(raw_point))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":649,"status":"ok","timestamp":1649677882590,"user":{"displayName":"wang peng","userId":"03125119609388965113"},"user_tz":-480},"id":"G2eiagzUVl6q","outputId":"3055741d-eb2e-4052-e79b-6c69efb54d89"},"outputs":[{"data":{"text/plain":["[-0.1318268087514801]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["MSC"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOrl8z1dSFx9oBOBHD2Z5r0","collapsed_sections":["u5V1GRk5A-qw"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
